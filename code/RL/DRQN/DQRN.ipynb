{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load the game environment </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 13,  5,  0,  6,  3,  4, 10])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import heapq\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BikeNet():\n",
    "    def __init__(self, N, A, R, Q, repair, warmup_time, run_time, start_position=0):\n",
    "        self.N = N\n",
    "        self.A = A\n",
    "        self.R = R\n",
    "        self.Q = Q\n",
    "        self.repair = repair\n",
    "        self.warmup_time = warmup_time\n",
    "        self.run_time = run_time\n",
    "        self.time_limit = warmup_time + run_time\n",
    "        self.car = start_position\n",
    "        self.edge = int(self.A**0.5)\n",
    "        self.areas = list(range(A + 1))\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.T = 0\n",
    "        self.carrier_position = self.car\n",
    "        self.state = [int(self.N / self.A)] * self.A + [0] * self.A\n",
    "        self.scheduler = []\n",
    "        heapq.heapify(self.scheduler)\n",
    "        for i in range(self.A):\n",
    "            heapq.heappush(self.scheduler, [random.expovariate(self.R[i][0]), -1, i])\n",
    "        heapq.heapify(self.scheduler)\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def warmup(self):\n",
    "        s = self.reset()\n",
    "\n",
    "        while self.T < self.warmup_time:\n",
    "            action = random.randint(0,3)\n",
    "            result = self.step(action, 0)\n",
    "\n",
    "        self.T = self.warmup_time\n",
    "        return np.array(env.state)\n",
    "\n",
    "    def get_distance(self, start, end):\n",
    "        dist = abs(start % (self.edge) - end % (self.edge)) + abs(\n",
    "            start // (self.edge) - end // (self.edge))\n",
    "        return dist\n",
    "    \n",
    "    def step(self, action, not_warm):\n",
    "        \n",
    "        cus = 0\n",
    "        # time for carrier to take the action and repair one bicycle\n",
    "        dist = self.get_distance(self.carrier_position, action)\n",
    "        t = dist * 2\n",
    "        if self.state[action + self.A] > 0:\n",
    "            t_cursor = self.T + t + self.repair\n",
    "        else:\n",
    "            t_cursor = self.T + t\n",
    "\n",
    "        event = self.scheduler[0]\n",
    "        self.T, kind, location = event[0], event[1], event[2]\n",
    "\n",
    "        # update the atate of QN during the tansformation time\n",
    "        while self.T < t_cursor:\n",
    "            # 车到达\n",
    "            if kind == 1:\n",
    "                self.state[location] += 1\n",
    "                heapq.heappop(self.scheduler)\n",
    "            else:# 顾客到达\n",
    "                if self.state[location] == 0:  # 但没车\n",
    "                    #rewards -= 1\n",
    "                    heapq.heappop(self.scheduler)\n",
    "                else:\n",
    "                    target = np.random.choice(self.areas, 1, p=self.Q[location])[0]\n",
    "                    if target == self.A:  # 顾客到达，发现是坏车\n",
    "                        self.state[location] -= 1\n",
    "                        self.state[location + self.A] += 1\n",
    "                        continue\n",
    "                    else:  # 顾客到达，顺利骑行\n",
    "                        cus += 1\n",
    "                        self.state[location] -= 1\n",
    "                        heapq.heappop(self.scheduler)\n",
    "                        next_time = random.expovariate(self.R[location][1]) + self.T\n",
    "                        heapq.heappush(self.scheduler, [next_time, 1, target])\n",
    "                next_time = random.expovariate(self.R[location][0]) + self.T\n",
    "                heapq.heappush(self.scheduler, [next_time, -1, location])\n",
    "\n",
    "            if self.scheduler:\n",
    "                event = self.scheduler[0]\n",
    "                self.T, kind, location = event[0], event[1], event[2]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if not_warm == 1:\n",
    "            if self.state[action + self.A] > 0:\n",
    "                self.state[self.carrier_position] += 1\n",
    "                self.state[self.carrier_position + self.A] -= 1\n",
    "\n",
    "        self.carrier_position = action\n",
    "        self.T = t_cursor\n",
    "\n",
    "        s_ = np.array(self.state)\n",
    "\n",
    "        #if self.T <= self.time_limit and self.scheduler:\n",
    "        return s_, cus, 0, t\n",
    "        #else:\n",
    "        #    return s_, cus, 1, t\n",
    "random.seed(0)\n",
    "N = 80  # total number of bikes in the QN\n",
    "A = 4  # A for areas, indicates the number of areas and the action space\n",
    "R = {}  # [customer_arrval, ride]\n",
    "for i in range(A): R[i] = [0.25 * i+0.5, 0.2]\n",
    "Q = [[0.15,0.3,0.3,0.15,0.1],\n",
    "     [0.3,0.15,0.15,0.3,0.1],\n",
    "     [0.3,0.15,0.15,0.3,0.1],\n",
    "     [0.15,0.3,0.3,0.15,0.1]]\n",
    "# Q = [[0,0.9,0.1], [0.9,0,0.1]]\n",
    "t_repair = 2\n",
    "warmup_time = 60\n",
    "run_time = 180\n",
    "\n",
    "env = BikeNet(N=N,\n",
    "              A=A,\n",
    "              R=R,\n",
    "              Q=Q,\n",
    "              repair=t_repair,\n",
    "              warmup_time=warmup_time,\n",
    "              run_time=run_time,\n",
    "              start_position=0)\n",
    "env.warmup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gridworld import gameEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gameEnv(partial=True,size=9)\n",
    "prev_state = env.reset()\n",
    "type(prev_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set Device </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Training Deep Recurrent Q Network (LSTM) </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Hyper-parameters </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 8\n",
    "OUT_SIZE = 4\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEP = 60\n",
    "GAMMA = 1.0\n",
    "INITIAL_EPSILON = 1.0\n",
    "FINAL_EPSILON = 0.1\n",
    "TOTAL_EPSIODES = 60\n",
    "#MAX_STEPS = 50\n",
    "MAX_STEP = 180\n",
    "MEMORY_SIZE = 64#000\n",
    "UPDATE_FREQ = 5\n",
    "PERFORMANCE_SAVE_INTERVAL = 500\n",
    "TARGET_UPDATE_FREQ = 20 #steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Build Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (lstm_layer): LSTM(8, 128, batch_first=True)\n",
      "  (adv): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (val): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,out_size):\n",
    "        super(Network,self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "        \n",
    "        self.lstm_layer = nn.LSTM(input_size=self.input_size,hidden_size=128,num_layers=1,batch_first=True)\n",
    "        self.adv = nn.Linear(in_features=128,out_features=self.out_size)\n",
    "        self.val = nn.Linear(in_features=128,out_features=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self,x,bsize,time_step,hidden_state,cell_state):\n",
    "        #x = x.view(bsize*time_step,1,self.input_size,self.input_size)\n",
    "        \n",
    "#         conv_out = self.conv_layer1(x)\n",
    "#         conv_out = self.relu(conv_out)\n",
    "#         conv_out = self.conv_layer2(conv_out)\n",
    "#         conv_out = self.relu(conv_out)\n",
    "#         conv_out = self.conv_layer3(conv_out)\n",
    "#         conv_out = self.relu(conv_out)\n",
    "#         conv_out = self.conv_layer4(conv_out)\n",
    "#         conv_out = self.relu(conv_out)\n",
    "        \n",
    "        x = x.view(bsize,-1,8)\n",
    "        \n",
    "        lstm_out = self.lstm_layer(x,(hidden_state,cell_state))\n",
    "        out = lstm_out[0][:,-1,:]\n",
    "        h_n = lstm_out[1][0]\n",
    "        c_n = lstm_out[1][1]\n",
    "        \n",
    "        adv_out = self.adv(out)\n",
    "        val_out = self.val(out)\n",
    "        \n",
    "        qout = val_out.expand(bsize,self.out_size) + (adv_out - adv_out.mean(dim=1).unsqueeze(dim=1).expand(bsize,self.out_size))\n",
    "        \n",
    "        return qout, (h_n,c_n)\n",
    "    \n",
    "    def init_hidden_states(self,bsize):\n",
    "        h = torch.zeros(1,bsize,128).float().to(device)\n",
    "        c = torch.zeros(1,bsize,128).float().to(device)\n",
    "        \n",
    "        return h,c\n",
    "    \n",
    "main_model = Network(input_size=INPUT_DIM,out_size=OUT_SIZE).to(device)\n",
    "print(main_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Experience Replay </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    \n",
    "    def __init__(self,memsize):\n",
    "        self.memsize = memsize\n",
    "        self.memory = deque(maxlen=self.memsize)\n",
    "    \n",
    "    def add_episode(self,epsiode):\n",
    "        self.memory.append(epsiode)\n",
    "    \n",
    "    def get_batch(self,bsize,time_step):\n",
    "        sampled_epsiodes = random.sample(self.memory,bsize)\n",
    "        batch = []\n",
    "        for episode in sampled_epsiodes:\n",
    "            point = np.random.randint(0,len(episode)+1-time_step)\n",
    "            batch.append(episode[point:point+time_step])\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Preprocess Image </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    return rgb2gray(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd367714080>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(preprocess_image(prev_state),cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Save Dictionary Function </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> DQN With LSTM </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:04<00:00, 17.74it/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populated with 64 Episodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 28/60 [02:06<02:24,  4.53s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4d500c8560ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mtarget_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTIME_STEP\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGAMMA\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_next_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m             \u001b[0mQ_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_current_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTIME_STEP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcell_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m             \u001b[0mQ_s_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_acts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTIME_STEP\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-f9dc79b15503>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, bsize, time_step, hidden_state, cell_state)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mlstm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mh_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/python/miniconda3/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             result = _impl(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 179\u001b[0;31m                            self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mem = Memory(memsize=MEMORY_SIZE)\n",
    "\n",
    "main_model = Network(input_size=INPUT_DIM,out_size=OUT_SIZE).float().to(device)\n",
    "target_model = Network(input_size=INPUT_DIM,out_size=OUT_SIZE).float().to(device)\n",
    "\n",
    "target_model.load_state_dict(main_model.state_dict())\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adagrad(main_model.parameters(),lr=0.00025)\n",
    "\n",
    "\n",
    "# Fill memory\n",
    "for i in tqdm(range(0,MEMORY_SIZE)):\n",
    "    \n",
    "    prev_state = env.warmup()\n",
    "    processed_prev_state = prev_state\n",
    "    #processed_prev_state = preprocess_image(prev_state)\n",
    "    step_count = 0\n",
    "    local_memory = []\n",
    "    \n",
    "    while step_count < MAX_STEP:\n",
    "        \n",
    "        step_count +=1\n",
    "        action = np.random.randint(0,4)\n",
    "        #if not env.scheduler: break\n",
    "        next_state,reward,done,t = env.step(action,1)\n",
    "        #time_count += t\n",
    "        processed_next_state = next_state #preprocess_image(next_state)\n",
    "        \n",
    "        local_memory.append((processed_prev_state,action,reward,processed_next_state))\n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "    \n",
    "    mem.add_episode(local_memory)\n",
    "        \n",
    "print('Populated with %d Episodes'%(len(mem.memory)))\n",
    "\n",
    "# Start Algorithm\n",
    "epsilon = INITIAL_EPSILON\n",
    "loss_stat = []\n",
    "reward_stat = []\n",
    "total_steps = 0\n",
    "\n",
    "for episode in tqdm(range(0,TOTAL_EPSIODES)):\n",
    "    \n",
    "    prev_state = env.warmup()\n",
    "    processed_prev_state = prev_state #preprocess_image(prev_state)\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    #time_count = 0\n",
    "    local_memory = []\n",
    "\n",
    "    hidden_state, cell_state = main_model.init_hidden_states(bsize=1)\n",
    "    \n",
    "    while step_count < MAX_STEP:\n",
    "        \n",
    "        step_count +=1\n",
    "        total_steps +=1\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            #torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            #model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            action = random.randint(0,3)\n",
    "            #hidden_state = model_out[1][0]\n",
    "            #cell_state = model_out[1][1]\n",
    "            \n",
    "        else:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            out = model_out[0]\n",
    "            action = int(torch.argmax(out[0]))\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "        \n",
    "        next_state,reward,done,t = env.step(action,1)\n",
    "        #time_count += t\n",
    "        total_reward += reward\n",
    "        processed_next_state = next_state #preprocess_image(next_state)\n",
    "        \n",
    "        local_memory.append((processed_prev_state,action,reward,processed_next_state))\n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "        \n",
    "        \n",
    "        if (total_steps % TARGET_UPDATE_FREQ) == 0:\n",
    "            target_model.load_state_dict(main_model.state_dict())\n",
    "       \n",
    "        if (total_steps % UPDATE_FREQ) == 0:\n",
    "            \n",
    "            hidden_batch, cell_batch = main_model.init_hidden_states(bsize=BATCH_SIZE)\n",
    "            \n",
    "            batch = mem.get_batch(bsize=BATCH_SIZE,time_step=TIME_STEP)\n",
    "            \n",
    "            current_states = []\n",
    "            acts = []\n",
    "            rewards = []\n",
    "            next_states = []\n",
    "            \n",
    "            for b in batch:\n",
    "                cs,ac,rw,ns = [],[],[],[]\n",
    "                for element in b:\n",
    "                    cs.append(element[0])\n",
    "                    ac.append(element[1])\n",
    "                    rw.append(element[2])\n",
    "                    ns.append(element[3])\n",
    "                current_states.append(cs)\n",
    "                acts.append(ac)\n",
    "                rewards.append(rw)\n",
    "                next_states.append(ns)\n",
    "            \n",
    "            current_states = np.array(current_states)\n",
    "            acts = np.array(acts)\n",
    "            rewards = np.array(rewards)\n",
    "            next_states = np.array(next_states)\n",
    "            \n",
    "            torch_current_states = torch.from_numpy(current_states).float().to(device)\n",
    "            torch_acts = torch.from_numpy(acts).long().to(device)\n",
    "            torch_rewards = torch.from_numpy(rewards).float().to(device)\n",
    "            torch_next_states = torch.from_numpy(next_states).float().to(device)\n",
    "            \n",
    "            \n",
    "            Q_next,_ = target_model.forward(torch_next_states,bsize=BATCH_SIZE,time_step=TIME_STEP,hidden_state=hidden_batch,cell_state=cell_batch)\n",
    "            Q_next_max,__ = Q_next.detach().max(dim=1)\n",
    "            target_values = torch_rewards[:,TIME_STEP-1] + (GAMMA * Q_next_max)\n",
    "            \n",
    "            Q_s, _ = main_model.forward(torch_current_states,bsize=BATCH_SIZE,time_step=TIME_STEP,hidden_state=hidden_batch,cell_state=cell_batch)\n",
    "            Q_s_a = Q_s.gather(dim=1,index=torch_acts[:,TIME_STEP-1].unsqueeze(dim=1)).squeeze(dim=1)\n",
    "            \n",
    "            \n",
    "            loss = criterion(Q_s_a,target_values)\n",
    "            \n",
    "            #  save performance measure\n",
    "            loss_stat.append(loss.item())\n",
    "            \n",
    "            # make previous grad zero\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # backward\n",
    "            loss.backward()\n",
    "            \n",
    "            # update params\n",
    "            optimizer.step()\n",
    "        if done: break\n",
    "\n",
    "    # save performance measure\n",
    "    reward_stat.append(total_reward)\n",
    "    \n",
    "    mem.add_episode(local_memory)\n",
    "\n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/TOTAL_EPSIODES\n",
    "    \n",
    "    if (episode + 1)% PERFORMANCE_SAVE_INTERVAL == 0:\n",
    "        perf = {}\n",
    "        perf['loss'] = loss_stat\n",
    "        perf['total_reward'] = reward_stat\n",
    "        save_obj(name='LSTM_POMDP_V4',obj=perf)\n",
    "    \n",
    "    \n",
    "    #print('Episode : ',episode+1,'Epsilon : ',epsilon,'Reward : ',total_reward,)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[362,\n",
       " 488,\n",
       " 459,\n",
       " 431,\n",
       " 410,\n",
       " 362,\n",
       " 384,\n",
       " 399,\n",
       " 402,\n",
       " 435,\n",
       " 419,\n",
       " 442,\n",
       " 384,\n",
       " 465,\n",
       " 472,\n",
       " 432,\n",
       " 437,\n",
       " 546,\n",
       " 287,\n",
       " 478,\n",
       " 432,\n",
       " 434,\n",
       " 381,\n",
       " 312,\n",
       " 449,\n",
       " 473,\n",
       " 459,\n",
       " 460,\n",
       " 400,\n",
       " 447]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Save Primary Network Weights </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(main_model.state_dict(),'data/LSTM_POMDP_V4_WEIGHTS.torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Testing Policy </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Load Primary Network Weights </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = torch.load('data/LSTM_POMDP_V4_WEIGHTS.torch')\n",
    "main_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start Testing\n",
    "epsilon = INITIAL_EPSILON\n",
    "FINAL_EPSILON = 0.01\n",
    "TOTAL_EPSIODES = 10000\n",
    "loss_stat = []\n",
    "reward_stat = []\n",
    "total_steps = 0\n",
    "\n",
    "for episode in range(0,TOTAL_EPSIODES):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    processed_prev_state = preprocess_image(prev_state)\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    hidden_state, cell_state = main_model.init_hidden_states(bsize=1)\n",
    "    \n",
    "    while step_count < MAX_STEPS:\n",
    "        \n",
    "        step_count +=1\n",
    "        total_steps +=1\n",
    "        \n",
    "        if np.random.rand(1) < epsilon:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            action = np.random.randint(0,4)\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "            \n",
    "        else:\n",
    "            torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "            model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "            out = model_out[0]\n",
    "            action = int(torch.argmax(out[0]))\n",
    "            hidden_state = model_out[1][0]\n",
    "            cell_state = model_out[1][1]\n",
    "        \n",
    "        next_state,reward,done = env.step(action)\n",
    "        total_reward += reward\n",
    "        processed_next_state = preprocess_image(next_state)\n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "            \n",
    "    # save performance measure\n",
    "    reward_stat.append(total_reward)\n",
    "    \n",
    "    if epsilon > FINAL_EPSILON:\n",
    "        epsilon -= (INITIAL_EPSILON - FINAL_EPSILON)/TOTAL_EPSIODES\n",
    "    \n",
    "    if (episode + 1)% PERFORMANCE_SAVE_INTERVAL == 0:\n",
    "        perf = {}\n",
    "        perf['loss'] = loss_stat\n",
    "        perf['total_reward'] = reward_stat\n",
    "        save_obj(name='LSTM_POMDP_V4_TEST',obj=perf)\n",
    "    \n",
    "    print('Episode : ',episode+1,'Epsilon : ',epsilon,'Reward : ',total_reward)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align='center'> Create Policy GIF </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Collect Frames Of an Episode Using Trained Network </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "full_env = gameEnv(partial=False,size=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward : 3\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "local_frames = []\n",
    "random.seed(110)\n",
    "np.random.seed(110)\n",
    "\n",
    "for episode in range(0,1):\n",
    "    \n",
    "    prev_state = env.reset()\n",
    "    random.seed(110)\n",
    "    np.random.seed(110)\n",
    "    full_env_prev = full_env.reset()\n",
    "    \n",
    "    frames.append(full_env_prev)\n",
    "    local_frames.append(prev_state)\n",
    "    processed_prev_state = preprocess_image(prev_state)\n",
    "    total_reward = 0\n",
    "    step_count = 0\n",
    "    \n",
    "    hidden_state, cell_state = main_model.init_hidden_states(bsize=1)\n",
    "    \n",
    "    while step_count < MAX_STEPS:\n",
    "        \n",
    "        step_count +=1\n",
    "        \n",
    "        torch_x = torch.from_numpy(processed_prev_state).float().to(device)\n",
    "        model_out = main_model.forward(torch_x,bsize=1,time_step=1,hidden_state=hidden_state,cell_state=cell_state)\n",
    "        out = model_out[0]\n",
    "        action = int(torch.argmax(out[0]))\n",
    "        hidden_state = model_out[1][0]\n",
    "        cell_state = model_out[1][1]\n",
    "        \n",
    "        next_state, reward, d = env.step(action)\n",
    "        full_env_next,r,g = full_env.step(action)\n",
    "        total_reward += reward\n",
    "        frames.append(full_env_next)\n",
    "        local_frames.append(next_state)\n",
    "        processed_next_state = preprocess_image(next_state)\n",
    "        \n",
    "        \n",
    "        prev_state = next_state\n",
    "        processed_prev_state = processed_next_state\n",
    "\n",
    "print('Total Reward : %d'%(total_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Frames to GIF </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayank/.conda/envs/myenv/lib/python3.6/site-packages/ipykernel/__main__.py:7: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n",
      "/home/mayank/.conda/envs/myenv/lib/python3.6/site-packages/ipykernel/__main__.py:8: DeprecationWarning: `imresize` is deprecated!\n",
      "`imresize` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``skimage.transform.resize`` instead.\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "from scipy.misc import imresize\n",
    "resized_frames = []\n",
    "resized_local_frames = []\n",
    "\n",
    "for i in range(0,len(frames)):\n",
    "    resized_frames.append(imresize(frames[i],(256,256)))\n",
    "    resized_local_frames.append(imresize(local_frames[i],(256,256)))\n",
    "\n",
    "imageio.mimsave('data/GIFs/LSTM_SIZE_9_frames.gif',resized_frames,fps=3)\n",
    "imageio.mimsave('data/GIFs/LSTM_SIZE_9_local.gif',resized_local_frames,fps=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
