{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BikeNet' object has no attribute 'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b158a34a0b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    337\u001b[0m                       \u001b[0;31m# output_graph=True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                       )\n\u001b[0;32m--> 339\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;31m#RL.plot_cost()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BikeNet' object has no attribute 'a'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This part of code is the Deep Q Network (DQN) brain.\n",
    "view the tensorboard picture about this DQN structure on: https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/4-3-DQN3/#modification\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: r1.2\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "import random\n",
    "import heapq\n",
    "from heapq import heapqpop\n",
    "from heapq import heapqpush\n",
    "\n",
    "\n",
    "# np.random.seed(1)\n",
    "# USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.line1 = nn.Linear(num_inputs, 64)\n",
    "        self.line2 = nn.Linear(64, 32)\n",
    "        self.line3 = nn.Linear(32, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.line1(x))\n",
    "        x = F.relu(self.line2(x))\n",
    "        x = self.line3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Train():\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=300,\n",
    "            memory_size=500,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self.target_net = DQN(n_features, n_actions)\n",
    "        self.eval_net = DQN(n_features, n_actions)\n",
    "        #self.eval_net.load_state_dict(torch.load('C:/Rebalancing/data/result/pytorchmodel/params.pkl'))\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adagrad(self.eval_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            actions_value = self.eval_net(Variable(torch.from_numpy(observation).float())).detach().numpy()\n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            # self.sess.run(self.target_replace_op)\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            # print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        self.s = Variable(torch.from_numpy(batch_memory[:, :self.n_features]).float(), requires_grad=True)\n",
    "        self.a = Variable(torch.from_numpy(batch_memory[:, self.n_features]).long())\n",
    "        self.r = Variable(torch.from_numpy(batch_memory[:, self.n_features + 1]).float())\n",
    "        self.s_ = Variable(torch.from_numpy(batch_memory[:, -self.n_features:]).float())\n",
    "\n",
    "\n",
    "        current_Q_values = self.eval_net(self.s).gather(1, self.a.unsqueeze(1)).view(-1)\n",
    "        next_Q_values = self.target_net(self.s_).detach().max(1)[0]\n",
    "        # Compute the target of the current Q values\n",
    "        target_Q_values = self.r + (self.gamma * next_Q_values)\n",
    "        # Compute Bellman error\n",
    "        loss = self.criterion(current_Q_values, target_Q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perfom the update\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     DQN = DeepQNetwork(3,4, output_graph=True)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Area():\n",
    "    def __init__(self, n, a_id):\n",
    "        self.a_id = a_id\n",
    "        self.normal_bike = n\n",
    "        self.broken_bike = 0\n",
    "\n",
    "    def move(self):\n",
    "        self.normal_bike -= 1\n",
    "        self.broken_bike += 1\n",
    "        \n",
    "    def repair(self):\n",
    "        self.normal_bike += 1\n",
    "        self.broken_bike -= 1\n",
    "\n",
    "\n",
    "\n",
    "class BikeNet():\n",
    "    def __init__(self, N, R, A, Q, repair, P, time_limit):\n",
    "        self.N = N\n",
    "        self.R = R\n",
    "        self.A = A\n",
    "        self.Q = Q\n",
    "        self.area = list(range(self.A+1))\n",
    "        self.repair = repair\n",
    "        self.P = P\n",
    "        self.time_limit = time_limit\n",
    "        self.reset()\n",
    "        self.trans = {}\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        # initiation of instances of Area and scheduler\n",
    "        self.T = 0\n",
    "        self.carrier_position = 0\n",
    "        self.scheduler = []\n",
    "        self.s = [int(self.N/self.A)] * self.A + [0]*self.A\n",
    "        for i in range(A):\n",
    "            self.scheduler.append([random.expovariate(self.R[i][0]), 1, i])\n",
    "        heapq.heapify(self.scheduler)\n",
    "\n",
    "        return self.s.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        # time for carrier to take the action and repair one bicycle\n",
    "        t = (abs(self.carrier_position % 3 - action % 3) + abs(self.carrier_position // 3 - action // 3)) *0.5 + self.repair\n",
    "        t_cursor = self.T + t\n",
    "        self.carrier_position = action\n",
    "        reward = 0\n",
    "        self.T = self.scheduler[0][0]\n",
    "\n",
    "        # update the atate of QN during the tansformation time\n",
    "        while self.T < t_cursor:\n",
    "            event = heapqpop(self.scheduler)\n",
    "            kind, place = event[1], event[2]\n",
    "            if kind == 1:\n",
    "                if self.s[place] == 0:\n",
    "                    # this is a loss\n",
    "                    reward -= 1\n",
    "                    next_event = [self.T + random.expovariate(self.R[place][0]), 1, place]\n",
    "                    heapq.heappush(self.scheduler, next_event)\n",
    "                else:\n",
    "                    target = random.choices(self.area, self.Q[place], k=1)[0]\n",
    "                    if target == self.A:\n",
    "                        self.s[place] -= 1\n",
    "                        self.s[place+self.A] += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.s[place] -= 1\n",
    "                        next_event1 = [self.T + random.expovariate(self.R[place][1]), 2, target]\n",
    "                        next_event2 = [self.T + random.expovariate(self.R[place][0]), 1, place]\n",
    "                        heapq.heappush(self.scheduler, next_event1)\n",
    "                        heapq.heappush(self.scheduler, next_event2)\n",
    "            else:\n",
    "                self.s[place] += 1\n",
    "            heapq.heappop(self.scheduler)\n",
    "            \n",
    "        self.carrier_position = action\n",
    "        if self.s[action+self.A]>0:\n",
    "            self.s[action+self.A] -= 1\n",
    "            self.s[action] += 1\n",
    "        s_ = self.s.copy()\n",
    "\n",
    "        self.T = t_cursor\n",
    "        if self.T < self.time_limit:\n",
    "            return s_, reward, 0\n",
    "        else:\n",
    "            return s_, reward, 1\n",
    "\n",
    "# from maze_env import Maze\n",
    "# from RL_brain import DeepQNetwork\n",
    "from tqdm import tqdm\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "@jit\n",
    "def simulate():\n",
    "    n_episodes = 10\n",
    "    result = []\n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        step = 0\n",
    "        sum_r = 0\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "        action = 0\n",
    "        #observation = np.array(int(N/A)) #devide all the normal bikes to all the areas evenly at the beginning\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            #env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            if not env.s[action+env.A]:\n",
    "            #action = RL.choose_action(observation)\n",
    "                action = (action+1)%A\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            #RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "            #if (step > 200) and (step % 5 == 0):\n",
    "            #    RL.learn()\n",
    "            #RL.learn()\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "            #step += 1\n",
    "            sum_r += reward\n",
    "            \n",
    "        result.append([episode, sum_r])\n",
    "\n",
    "    # end of game\n",
    "    print('learning over')\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    np.random.seed(1)\n",
    "    N = 80 #total number of bikes in the QN\n",
    "    A = 4 #A for areas, indicates the aumber of areas and the action space\n",
    "    R = {}  # [customer_arrval, ride]\n",
    "    for i in range(A): R[i] = [1.0, 0.5]\n",
    "    Q = [np.random.rand(A) for i in range(A)]\n",
    "    Q = [q / sum(q)*0.99 for q in Q]\n",
    "    Q = [np.append(q, 0.01) for q in Q]\n",
    "    #Q = [[0,0.9,0.1], [0.9,0,0.1]]\n",
    "    t_repair = 5\n",
    "    P = 0\n",
    "    time_limit = 180\n",
    "\n",
    "    env = BikeNet(N, R, A, Q, t_repair, P, time_limit)\n",
    "    \n",
    "    RL = Train(A, 2*A,\n",
    "                      learning_rate=0.01,\n",
    "                      reward_decay=0.9,\n",
    "                      e_greedy=0.9,\n",
    "                      replace_target_iter=200,\n",
    "                      memory_size=2000,\n",
    "                      # output_graph=True\n",
    "                      )\n",
    "    output = simulate()\n",
    "    #RL.plot_cost()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=18):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch.optim as optim\n",
    "\n",
    "from dqn_model import DQN\n",
    "from dqn_learn import OptimizerSpec, dqn_learing\n",
    "from utils.gym import get_env, get_wrapper_by_name\n",
    "from utils.schedule import LinearSchedule\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "LEARNING_STARTS = 50000\n",
    "LEARNING_FREQ = 4\n",
    "FRAME_HISTORY_LEN = 4\n",
    "TARGER_UPDATE_FREQ = 10000\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01\n",
    "\n",
    "def main(env, num_timesteps):\n",
    "\n",
    "    def stopping_criterion(env):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "\n",
    "    optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    "    )\n",
    "\n",
    "    exploration_schedule = LinearSchedule(1000000, 0.1)\n",
    "\n",
    "    dqn_learing(\n",
    "        env=env,\n",
    "        q_func=DQN,\n",
    "        optimizer_spec=optimizer_spec,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        learning_freq=LEARNING_FREQ,\n",
    "        frame_history_len=FRAME_HISTORY_LEN,\n",
    "        target_update_freq=TARGER_UPDATE_FREQ,\n",
    "    )\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get Atari games.\n",
    "    benchmark = gym.benchmark_spec('Atari40M')\n",
    "\n",
    "    # Change the index to select a different game.\n",
    "    task = benchmark.tasks[3]\n",
    "\n",
    "    # Run training\n",
    "    seed = 0 # Use a seed of zero (you may want to randomize the seed!)\n",
    "    env = get_env(task, seed)\n",
    "\n",
    "    main(env, task.max_timesteps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
