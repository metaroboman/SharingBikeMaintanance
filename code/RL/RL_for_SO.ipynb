{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here documents different version of PyTorch RL for simulation optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class BikeNet with R as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.9997322401401938, 0.5016355238016227)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.exponential(2.0, 100000)\n",
    "b = [random.expovariate(2.0) for x in range(100000)]\n",
    "np.average(a), np.average(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1,2], 1, [0.1, 0.9])==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choices(list(range(5)), k=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 3, 23,  7, 15,  7,  2, 11,  2]), 0, 0)\n",
      "(array([13, 15,  3, 17,  2,  5, 14,  3]), 0, 0)\n",
      "(array([ 6, 17,  7, 20,  3,  3,  5,  3]), 0, 0)\n",
      "(array([15, 17,  6, 21,  1,  3,  7,  0]), 0, 0)\n",
      "(array([18, 18, 11, 14,  1,  1,  5,  1]), 0, 0)\n",
      "(array([18, 20,  5, 19,  1,  0, 12,  0]), 0, 0)\n",
      "(array([11, 17,  5, 20,  3,  1, 11,  3]), 0, 0)\n",
      "(array([15, 11,  6, 23,  3,  4, 14,  0]), 0, 0)\n",
      "(array([21,  5,  0, 21,  1,  9, 13,  1]), -2, 0)\n",
      "(array([14, 20,  6, 19,  2,  1,  8,  3]), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "from heapq import heapqpop\n",
    "from heapq import heapqpush\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class BikeNet():\n",
    "    def __init__(self, N, R, A, Q, repair, time_limit, start_position=0):\n",
    "        self.N = N\n",
    "        self.R = R\n",
    "        self.A = A\n",
    "        self.areas = list(range(A + 1))\n",
    "        self.Q = Q\n",
    "        self.repair = repair\n",
    "        self.time_limit = time_limit\n",
    "        self.car = start_position\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.T = 0\n",
    "        self.carrier_position = self.car\n",
    "        self.state = [int(self.N/self.A)] * self.A + [0]*self.A\n",
    "        self.scheduler = []\n",
    "        heapq.heapify(self.scheduler)\n",
    "        for i in range(A):\n",
    "            heappush(self.scheduler, [random.expovariate(self.R[i][0]), 1, i])\n",
    "        heapq.heapify(self.scheduler)\n",
    "        return np.array(self.state.copy())\n",
    "\n",
    "    def step(self, action):\n",
    "        rewards = 0\n",
    "        # time for carrier to take the action and repair one bicycle\n",
    "        t = (abs(self.carrier_position % (self.A - 1) - action % (self.A - 1)) + abs(\n",
    "            self.carrier_position // (self.A - 1) - action // (self.A - 1))) * 0.5\n",
    "        if self.state[action+self.A] > 0:\n",
    "            self.state[self.carrier_position] += 1\n",
    "            self.state[self.carrier_position + 1] -= 1\n",
    "            t_cursor = self.T + t+self.repair\n",
    "        else: t_cursor = self.T + t\n",
    "            \n",
    "        event = self.scheduler[0]\n",
    "        self.T, kind, location = event[0], event[1], event[2]\n",
    "\n",
    "        # update the atate of QN during the tansformation time\n",
    "        while self.T < t_cursor:\n",
    "            # 车到达\n",
    "            if kind == 1:  \n",
    "                self.state[location] += 1\n",
    "            else:\n",
    "                # 顾客到达\n",
    "                if self.state[location] == 0:  #但没车\n",
    "                    rewards -= 1\n",
    "                else:\n",
    "                    target = np.random.choice(self.areas, 1, p=self.Q[location])[0]\n",
    "                    if target == self.A:  # 顾客到达，发现是坏车\n",
    "                        self.state[location] -= 1\n",
    "                        self.state[location + self.A] += 1\n",
    "                        continue\n",
    "                    else:  # 顾客到达，顺利骑行\n",
    "                        self.state[location] -= 1\n",
    "                        next_time = random.expovariate(self.R[location][0]) + self.T\n",
    "                        if next_time <= self.time_limit:\n",
    "                            heapq.heappush(self.scheduler, [next_time, 1, target])\n",
    "                next_time = random.expovariate(self.R[location][1]) + self.T\n",
    "                if next_time <= self.time_limit:\n",
    "                    heapq.heappush(self.scheduler, [next_time, -1, location])\n",
    "                    \n",
    "            heapq.heappop(self.scheduler)\n",
    "            if self.scheduler:\n",
    "                event = self.scheduler[0]\n",
    "                self.T, kind, location = event[0], event[1], event[2]\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        self.carrier_position = action\n",
    "        self.T = t_cursor\n",
    "        \n",
    "        s_ = np.array(self.state.copy())\n",
    "\n",
    "        if self.T <= self.time_limit and self.scheduler:\n",
    "            return s_, rewards, 0\n",
    "        else:\n",
    "            return s_, rewards, 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    random.seed(1)\n",
    "    N = 80  # total number of bikes in the QN\n",
    "    A = 4  # A for areas, indicates the number of areas and the action space\n",
    "    R = {} #[customer_arrval, ride]\n",
    "    for i in range(A): R[i] = [1.0, 0.5]\n",
    "    Q = [np.random.rand(A) for i in range(A)]\n",
    "    Q = [q / sum(q)*0.99 for q in Q]\n",
    "    Q = [np.append(q, 0.01) for q in Q]\n",
    "    t_repair = 0.5\n",
    "    time_limit = 10\n",
    "    start_position = 0\n",
    "    env = BikeNet(N, R, A, Q, t_repair, time_limit, start_position)\n",
    "    for i in range(10):\n",
    "        print(env.step(np.random.randint(0, A)))\n",
    "    env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class BikeNet with R as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([18, 18, 17, 18,  0,  2,  0,  0]), 0, 0)\n",
      "(array([18, 18, 18, 17,  1,  2,  0,  1]), 0, 0)\n",
      "(array([21, 19, 17, 14,  1,  1,  0,  1]), 0, 0)\n",
      "(array([15, 14, 19, 14,  1,  3,  0,  1]), 0, 0)\n",
      "(array([15, 10, 21, 12,  0,  9,  1,  2]), 0, 0)\n",
      "(array([16, 15, 16, 16,  4,  3,  0,  0]), 0, 0)\n",
      "(array([17, 19, 17, 15,  0,  2,  0,  0]), 0, 0)\n",
      "(array([16, 13, 20, 18,  2,  5,  0,  0]), 0, 0)\n",
      "(array([23, 18, 17, 13,  0,  1,  0,  2]), 0, 0)\n",
      "(array([16, 18, 14, 16,  0,  6,  1,  1]), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "class BikeNet():\n",
    "    def __init__(self, N, R, A, Q, repair, time_limit, start_position):\n",
    "        self.N = N\n",
    "        self.R = R\n",
    "        self.A = A\n",
    "        self.areas = list(range(A + 1))\n",
    "        self.Q = Q\n",
    "        self.repair = repair\n",
    "        self.time_limit = time_limit\n",
    "        self.car = start_position\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.T = 0\n",
    "        self.carrier_position = self.car\n",
    "        self.place = list(range(self.A))\n",
    "        self.time = sorted([np.random.exponential(1.0 / self.R.loc[i].cus_arr) for i in self.place], reverse=True)\n",
    "        self.type = [-1] * self.A\n",
    "        self.state = [int(self.N / self.A)] * self.A + [0] * self.A\n",
    "        self.scheduler = [[self.time[x], self.type[x], self.place[x]] for x in range(self.A)]\n",
    "        heapq.heapify(self.scheduler)\n",
    "        return np.array(self.state.copy())\n",
    "\n",
    "    def step(self, action):\n",
    "        rewards = 0\n",
    "        # time for carrier to take the action and repair one bicycle\n",
    "        t = (abs(self.carrier_position % (self.A - 1) - action % (self.A - 1)) + abs(\n",
    "            self.carrier_position // (self.A - 1) - action // (self.A - 1))) / \\\n",
    "            self.R.loc[0].ride + self.repair\n",
    "        t_cursor = self.T + t\n",
    "        event = self.scheduler[0]\n",
    "        self.T, kind, location = event[0], event[1], event[2]\n",
    "        self.carrier_position = action\n",
    "\n",
    "        # update the atate of QN during the tansformation time\n",
    "        while self.T < t_cursor:\n",
    "            if kind == 1:  # 车到达\n",
    "                self.state[location] += 1\n",
    "            else:\n",
    "                if self.state[location] == 0:  # 顾客到达，但没车\n",
    "                    rewards -= 1\n",
    "                else:\n",
    "                    target = np.random.choice(self.areas, 1, p=self.Q[location])[0]\n",
    "                    if target == self.A:  # 顾客到达，发现是坏车\n",
    "                        self.state[location] -= 1\n",
    "                        self.state[location + self.A] += 1\n",
    "                        continue\n",
    "                    else:  # 顾客到达，顺利骑行\n",
    "                        self.state[location] -= 1\n",
    "                        next_time = np.random.exponential(1.0 / self.R.loc[location].ride) + self.T\n",
    "                        if next_time <= self.time_limit:\n",
    "                            heapq.heappush(self.scheduler, [next_time, 1, target])\n",
    "                next_time = np.random.exponential(1.0 / self.R.loc[location].cus_arr) + self.T\n",
    "                if next_time <= self.time_limit:\n",
    "                    heapq.heappush(self.scheduler, [next_time, -1, location])\n",
    "            heapq.heappop(self.scheduler)\n",
    "            if self.scheduler:\n",
    "                event = self.scheduler[0]\n",
    "                self.T, kind, location = event[0], event[1], event[2]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        self.T = t_cursor\n",
    "        if self.state[self.carrier_position] > 0:\n",
    "            self.state[self.carrier_position] -= 1\n",
    "            self.state[self.carrier_position + self.A] += 1\n",
    "        s_ = np.array(self.state.copy())\n",
    "\n",
    "        if self.T <= self.time_limit and self.scheduler:\n",
    "            return s_, rewards, 0\n",
    "        else:\n",
    "            return s_, rewards, 1\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    np.random.seed(1)\n",
    "    N = 80  # total number of bikes in the QN\n",
    "    A = 4  # A for areas, indicates the number of areas and the action space\n",
    "    R = pd.DataFrame({'cus_arr': [1] * A, 'ride': [0.5] * A}, index=range(A))\n",
    "    Q = [np.random.rand(A + 1) for i in range(A)]\n",
    "    Q = [q / sum(q) for q in Q]\n",
    "    # Q = [[0,0.9,0.1], [0.9,0,0.1]]\n",
    "    t_repair = 5\n",
    "    P = 0\n",
    "    time_limit = 10\n",
    "    start_position = 0\n",
    "    env = BikeNet(N, R, A, Q, t_repair, time_limit, start_position)\n",
    "    #     RL = Train(A, 2*A,\n",
    "    #               learning_rate=0.01,\n",
    "    #               reward_decay=0.9,\n",
    "    #               e_greedy=0.9,\n",
    "    #               replace_target_iter=200,\n",
    "    #               memory_size=2000\n",
    "    #               # output_graph=True\n",
    "    #               )\n",
    "    #     net = DQN(8,4)\n",
    "    #     output = net(torch.randn(1,8))\n",
    "    for i in range(10):\n",
    "        print(env.step(np.random.randint(0, A)))\n",
    "        env.reset()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 ns ± 2.22 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n",
      "37.2 ns ± 1.39 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = list(range(100000000))\n",
    "%timeit a[0]\n",
    "%timeit a[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.95 µs ± 20.4 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n",
      "442 ns ± 3.81 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)\n",
      "82.9 ns ± 0.449 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(1, 100)\n",
    "t = torch.eye(100)[:, 98]\n",
    "b = np.array([0]*100)\n",
    "c = [0]*100\n",
    "%timeit a+t\n",
    "%timeit b[98]+=1\n",
    "%timeit c[98]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# np.random.seed(1)\n",
    "# USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# Deep Q Network off-policy\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.line1 = nn.Linear(num_inputs, 64)\n",
    "        self.line2 = nn.Linear(64, 32)\n",
    "        self.line3 = nn.Linear(32, num_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.line1(x))\n",
    "        x = F.relu(self.line2(x))\n",
    "        x = self.line3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Train():\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.01,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=300,\n",
    "            memory_size=500,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        # total learning step\n",
    "        self.learn_step_counter = 0\n",
    "\n",
    "        # initialize zero memory [s, a, r, s_]\n",
    "        self.memory = np.zeros((self.memory_size, n_features * 2 + 2))\n",
    "\n",
    "        # consist of [target_net, evaluate_net]\n",
    "        self.target_net = DQN(n_features, n_actions)\n",
    "        self.eval_net = DQN(n_features, n_actions)\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.SGD(self.eval_net.parameters(), lr=self.lr)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        # replace the old memory with new memory\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        # to have batch dimension when feed into tf placeholder\n",
    "        observation = observation[np.newaxis, :]\n",
    "\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # forward feed the observation and get q value for every actions\n",
    "            \n",
    "            action = np.argmax(actions_value)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        # check to replace target parameters\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            # self.sess.run(self.target_replace_op)\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "            # print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        # sample batch memory from all memory\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        self.s = Variable(torch.from_numpy(batch_memory[:, :self.n_features]).float(), requires_grad=True)\n",
    "        self.a = Variable(torch.from_numpy(batch_memory[:, self.n_features]).long())\n",
    "        self.r = Variable(torch.from_numpy(batch_memory[:, self.n_features + 1]).float(), requires_grad=True)\n",
    "        self.s_ = Variable(torch.from_numpy(batch_memory[:, -self.n_features:]).float(), requires_grad=True)\n",
    "\n",
    "        current_Q_values = self.eval_net(self.s).gather(1, self.a.unsqueeze(1)).view(-1)\n",
    "        next_Q_values = self.target_net(self.s_).detach().max(1)[0]\n",
    "        # Compute the target of the current Q values\n",
    "        target_Q_values = self.r + (self.gamma * next_Q_values)\n",
    "        # Compute Bellman error\n",
    "        loss = self.criterion(target_Q_values, current_Q_values)\n",
    "        #         # clip the bellman error between [-1 , 1]\n",
    "        #         clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
    "        #         # Note: clipped_bellman_delta * -1 will be right gradient\n",
    "        #         d_error = clipped_bellman_error * -1.0\n",
    "        #         # Clear previous gradients before backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        # run backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perfom the update\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # increasing epsilon\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original version of Class BikeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class Area():\n",
    "    def __init__(self, n, a_id):\n",
    "        self.a_id = a_id\n",
    "        self.normal_bike = n\n",
    "        self.broken_bike = 0\n",
    "\n",
    "    def move(self):\n",
    "        self.normal_bike -= 1\n",
    "        self.broken_bike += 1\n",
    "        \n",
    "    def repair(self):\n",
    "        self.normal_bike += 1\n",
    "        self.broken_bike -= 1\n",
    "\n",
    "\n",
    "def binaryInsert(target, events):\n",
    "    for event in events:\n",
    "        if event >= target[-1]:\n",
    "            target.append(event)\n",
    "        else:\n",
    "            l, mid, r = 0, int(len(target) / 2), len(target) - 1\n",
    "            while 1:\n",
    "                if r - l == 1:\n",
    "                    target.insert(r, event)\n",
    "                    break\n",
    "                else:\n",
    "                    if event > target[mid]:\n",
    "                        l = mid\n",
    "                        mid = int((r + l) / 2)\n",
    "                    else:\n",
    "                        r = mid\n",
    "                        mid = int((r + l) / 2)\n",
    "\n",
    "\n",
    "class BikeNet():\n",
    "    def __init__(self, N, R, A, Q, repair, time_limit, start_position):\n",
    "        self.N = N\n",
    "        self.R = R\n",
    "        self.A = A\n",
    "        self.Q = Q\n",
    "        self.repair = repair\n",
    "        self.time_limit = time_limit\n",
    "        self.carrier_position = start_position\n",
    "        self.reset()\n",
    "        self.trans = {}\n",
    "\n",
    "    def reset(self):\n",
    "        # self.__init__(self.N, self.R, self.A, self.Q, self.P, self.time_limit)\n",
    "        # stat, S = pd.DataFrame(columns=['type', 'place', 't']), 0\n",
    "        # loss, L = pd.DataFrame(columns=['place', 't']), 0\n",
    "        # broken, B = pd.DataFrame(columns=['place', 'ng', 'nb', 't']), 0\n",
    "\n",
    "        # initiation of instances of Area and scheduler\n",
    "        self.T = 0\n",
    "        self.scheduler = []\n",
    "        self.a = []  # list of instances of areas\n",
    "        self.s = np.array([(self.N / self.A) if i % 2 == 0 else 0 for i in range(2 * self.A)])\n",
    "        for i in range(A):\n",
    "            self.a.append(Area(self.N / self.A, i))\n",
    "            self.scheduler.append([np.random.exponential(1 / self.R.loc[i].cus_arr), 1, self.a[i]])\n",
    "        self.scheduler.sort()\n",
    "\n",
    "        return self.s.copy()\n",
    "\n",
    "    def step(self, action):\n",
    "        # time for carrier to take the action and repair one bicycle\n",
    "        t = (np.abs(self.carrier_position % 3 - action % 3) + np.abs(self.carrier_position // 3 - action // 3)) / \\\n",
    "            self.R.loc[0].ride + self.repair\n",
    "        t_cursor = self.T + t\n",
    "        self.carrier_position = action\n",
    "        reward = 0\n",
    "\n",
    "        # update the atate of QN during the tansformation time\n",
    "        while self.T < t_cursor:\n",
    "            self.T = self.scheduler[0][0]\n",
    "            if self.scheduler[0][1] == 1:\n",
    "                # stat.loc[S], S = [scheduler[0][1], scheduler[0][2].a_id, T], S+1\n",
    "                if self.scheduler[0][2].normal_bike == 0:\n",
    "                    # this is a loss\n",
    "                    reward -= 1\n",
    "                    # loss.loc[L], L = [scheduler[0][2].a_id, self.T], L+1\n",
    "                    event = [self.T + np.random.exponential(1 / self.R.loc[self.scheduler[0][2].a_id].cus_arr), 1,\n",
    "                             self.scheduler[0][2]]\n",
    "                    binaryInsert(self.scheduler, [event])\n",
    "                else:\n",
    "                    target = np.random.choice(np.arange(self.A + 1), 1, p=self.Q[self.scheduler[0][2].a_id])\n",
    "                    if target == self.A:\n",
    "                        # broken.loc[B], B = [self.scheduler[0][2].a_id, self.scheduler[0][2].normal_bike, self.scheduler[0][2].broken_bike, T], B+1\n",
    "                        self.scheduler[0][2].move()\n",
    "                        self.s[self.scheduler[0][2].a_id * 2], self.s[self.scheduler[0][2].a_id * 2 + 1] = \\\n",
    "                        self.scheduler[0][2].normal_bike, self.scheduler[0][2].broken_bike\n",
    "                        continue\n",
    "                    else:\n",
    "                        self.scheduler[0][2].normal_bike -= 1\n",
    "                        self.s[self.scheduler[0][2].a_id * 2] -= 1\n",
    "                        event1 = [self.T + np.random.exponential(1 / self.R.loc[self.scheduler[0][2].a_id].ride), 2,\n",
    "                                  self.a[target[0]]]\n",
    "                        event2 = [self.T + np.random.exponential(1 / self.R.loc[self.scheduler[0][2].a_id].cus_arr), 1,\n",
    "                                  self.scheduler[0][2]]\n",
    "                        binaryInsert(self.scheduler, [event1, event2])\n",
    "            else:\n",
    "                # stat.loc[S], S = [scheduler[0][1], scheduler[0][2].a_id, T], S+1\n",
    "                self.scheduler[0][2].normal_bike += 1\n",
    "                self.s[self.scheduler[0][2].a_id * 2] += 1\n",
    "            self.scheduler.pop(0)\n",
    "        \n",
    "        self.a[action].repair()\n",
    "        s_ = self.s.copy()\n",
    "\n",
    "        self.T = t_cursor\n",
    "        if self.T < self.time_limit:\n",
    "            return s_, reward, 0\n",
    "        else:\n",
    "            return s_, reward, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test BikeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([46.,  3., 53.,  1.,  8., 11., 42.,  1., 36.,  0., 36.,  7., 44.,\n",
      "        3., 23.,  5., 35.,  0.]), 0, 0)\n",
      "(array([32.,  9., 49.,  0., 32.,  4., 48.,  3., 29.,  1., 27.,  5., 37.,\n",
      "        7., 24.,  3., 46.,  1.]), 0, 0)\n",
      "(array([38.,  4., 51.,  2.,  0., 15., 50.,  0., 38.,  0., 40.,  4., 42.,\n",
      "        5., 26.,  5., 36.,  1.]), -2, 0)\n",
      "(array([26., 10., 47.,  4., 25.,  6., 55.,  4., 29.,  0., 26.,  9., 41.,\n",
      "        4., 34.,  5., 29.,  3.]), 0, 0)\n",
      "(array([41.,  2., 41.,  2., 25.,  4., 37.,  8., 23.,  3., 35.,  2., 51.,\n",
      "        1., 44.,  2., 34.,  3.]), 0, 0)\n",
      "(array([45.,  3., 45.,  1., 20.,  9., 27.,  5., 46.,  0., 34.,  5., 46.,\n",
      "        3., 30.,  2., 31.,  3.]), 0, 0)\n",
      "(array([43.,  1., 57.,  0., 22.,  2., 42.,  2., 26.,  1., 29.,  6., 39.,\n",
      "        4., 28.,  5., 45.,  1.]), 0, 0)\n",
      "(array([36.,  6., 44.,  0., 12., 18., 57.,  0., 38.,  0., 43.,  2., 26.,\n",
      "        7., 32.,  1., 33.,  2.]), 0, 0)\n",
      "(array([33.,  8., 49.,  2., 32.,  6., 37.,  1., 26.,  1., 25.,  7., 37.,\n",
      "        6., 36.,  4., 41.,  5.]), 0, 0)\n",
      "(array([39.,  5., 44.,  0., 30.,  3., 45.,  8., 35.,  0., 27.,  6., 33.,\n",
      "        3., 30.,  5., 44.,  0.]), 0, 0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    np.random.seed(1)\n",
    "    N = 360 #total number of bikes in the QN\n",
    "    A = 9 #A for areas, indicates the number of areas and the action space\n",
    "    R = pd.DataFrame({'cus_arr': [5] * A, 'ride': [10] * A}, index=range(A))\n",
    "    Q = [np.random.rand(A+1) for i in range(A)]\n",
    "    Q = [q / sum(q) for q in Q]\n",
    "    #Q = [[0,0.9,0.1], [0.9,0,0.1]]\n",
    "    t_repair = 5\n",
    "    P = 0\n",
    "    time_limit = 10\n",
    "    start_position = 0\n",
    "    env = BikeNet(N, R, A, Q, t_repair, time_limit, start_position)\n",
    "#     RL = Train(A, 2*A,\n",
    "#               learning_rate=0.01,\n",
    "#               reward_decay=0.9,\n",
    "#               e_greedy=0.9,\n",
    "#               replace_target_iter=200,\n",
    "#               memory_size=2000\n",
    "#               # output_graph=True\n",
    "#               )\n",
    "#     net = DQN(8,4)\n",
    "#     output = net(torch.randn(1,8))\n",
    "    for i in range(10):\n",
    "        #print(env.time)\n",
    "        print(env.step(np.random.randint(0, 9)))\n",
    "        env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0423, -0.0537, -0.1885,  0.0527]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def simulate():\n",
    "    n_episodes = 3\n",
    "    result = []\n",
    "    for episode in range(n_episodes):\n",
    "        sum_r = 0\n",
    "        step = 0\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "        # observation = np.array(int(N/A)) #devide all the normal bikes to all the areas evenly at the beginning\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            # env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.choose_action(observation)\n",
    "            # action = (action+1)%9\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "            RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "            if (step > 200) and (step % 5 == 0):\n",
    "                RL.learn()\n",
    "            #RL.learn()\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "            step += 1\n",
    "            sum_r += reward\n",
    "\n",
    "        result.append([episode, sum_r, env.T])\n",
    "\n",
    "    # end of game\n",
    "    print('learning over')\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    np.random.seed(10)\n",
    "    N = 80  # total number of bikes in the QN\n",
    "    A = 4  # A for areas, indicates the number of areas and the action space\n",
    "    R = pd.DataFrame({'cus_arr': [1] * A, 'ride': [0.5] * A}, index=range(A))\n",
    "    Q = [np.random.rand(A) for i in range(A)]\n",
    "    Q = [q / sum(q) * 0.99 for q in Q]\n",
    "    Q = [np.append(q, 0.01) for q in Q]\n",
    "    # Q = [[0,0.9,0.1], [0.9,0,0.1]]\n",
    "    t_repair = 5\n",
    "    P = 0\n",
    "    time_limit = 1800\n",
    "\n",
    "    env = BikeNet(N, R, A, Q, t_repair, P, time_limit)\n",
    "\n",
    "    RL = Train(A, 2 * A,\n",
    "                   learning_rate=0.01,\n",
    "                   reward_decay=0.9,\n",
    "                   e_greedy=0.9,\n",
    "                   replace_target_iter=200,\n",
    "                   memory_size=2000,\n",
    "                   # output_graph=True\n",
    "               )\n",
    "    output = simulate()\n",
    "    #print(output)\n",
    "    # RL.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from maze_env import Maze\n",
    "# from RL_brain import DeepQNetwork\n",
    "\n",
    "\n",
    "def run_maze():\n",
    "    step = 0\n",
    "    for episode in range(300):\n",
    "        # initial observation\n",
    "        observation = env.reset()\n",
    "\n",
    "        while True:\n",
    "            # fresh env\n",
    "            env.render()\n",
    "\n",
    "            # RL choose action based on observation\n",
    "            action = RL.choose_action(observation)\n",
    "\n",
    "            # RL take action and get next observation and reward\n",
    "            observation_, reward, done = env.step(action)\n",
    "\n",
    "            RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "            if (step > 200) and (step % 5 == 0):\n",
    "                RL.learn()\n",
    "\n",
    "            # swap observation\n",
    "            observation = observation_\n",
    "\n",
    "            # break while loop when end of this episode\n",
    "            if done:\n",
    "                break\n",
    "            step += 1\n",
    "\n",
    "    # end of game\n",
    "    print('game over')\n",
    "    env.destroy()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # maze game\n",
    "    env = Maze()\n",
    "    RL = DeepQNetwork(env.n_actions, env.n_features,\n",
    "                      learning_rate=0.01,\n",
    "                      reward_decay=0.9,\n",
    "                      e_greedy=0.9,\n",
    "                      replace_target_iter=200,\n",
    "                      memory_size=2000,\n",
    "                      # output_graph=True\n",
    "                      )\n",
    "    env.after(100, run_maze)\n",
    "    env.mainloop()\n",
    "    RL.plot_cost()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reinforcement learning maze example.\n",
    "Red rectangle:          explorer.\n",
    "Black rectangles:       hells       [reward = -1].\n",
    "Yellow bin circle:      paradise    [reward = +1].\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "if sys.version_info.major == 2:\n",
    "    import Tkinter as tk\n",
    "else:\n",
    "    import tkinter as tk\n",
    "\n",
    "UNIT = 40   # pixels\n",
    "MAZE_H = 4  # grid height\n",
    "MAZE_W = 4  # grid width\n",
    "\n",
    "\n",
    "class Maze(tk.Tk, object):\n",
    "    def __init__(self):\n",
    "        super(Maze, self).__init__()\n",
    "        self.action_space = ['u', 'd', 'l', 'r']\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.n_features = 2\n",
    "        self.title('maze')\n",
    "        self.geometry('{0}x{1}'.format(MAZE_H * UNIT, MAZE_H * UNIT))\n",
    "        self._build_maze()\n",
    "\n",
    "    def _build_maze(self):\n",
    "        self.canvas = tk.Canvas(self, bg='white',\n",
    "                           height=MAZE_H * UNIT,\n",
    "                           width=MAZE_W * UNIT)\n",
    "\n",
    "        # create grids\n",
    "        for c in range(0, MAZE_W * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = c, 0, c, MAZE_H * UNIT\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "        for r in range(0, MAZE_H * UNIT, UNIT):\n",
    "            x0, y0, x1, y1 = 0, r, MAZE_W * UNIT, r\n",
    "            self.canvas.create_line(x0, y0, x1, y1)\n",
    "\n",
    "        # create origin\n",
    "        origin = np.array([20, 20])\n",
    "\n",
    "        # hell\n",
    "        hell1_center = origin + np.array([UNIT * 2, UNIT])\n",
    "        self.hell1 = self.canvas.create_rectangle(\n",
    "            hell1_center[0] - 15, hell1_center[1] - 15,\n",
    "            hell1_center[0] + 15, hell1_center[1] + 15,\n",
    "            fill='black')\n",
    "        # hell\n",
    "        # hell2_center = origin + np.array([UNIT, UNIT * 2])\n",
    "        # self.hell2 = self.canvas.create_rectangle(\n",
    "        #     hell2_center[0] - 15, hell2_center[1] - 15,\n",
    "        #     hell2_center[0] + 15, hell2_center[1] + 15,\n",
    "        #     fill='black')\n",
    "\n",
    "        # create oval\n",
    "        oval_center = origin + UNIT * 2\n",
    "        self.oval = self.canvas.create_oval(\n",
    "            oval_center[0] - 15, oval_center[1] - 15,\n",
    "            oval_center[0] + 15, oval_center[1] + 15,\n",
    "            fill='yellow')\n",
    "\n",
    "        # create red rect\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "\n",
    "        # pack all\n",
    "        self.canvas.pack()\n",
    "\n",
    "    def reset(self):\n",
    "        self.update()\n",
    "        time.sleep(0.1)\n",
    "        self.canvas.delete(self.rect)\n",
    "        origin = np.array([20, 20])\n",
    "        self.rect = self.canvas.create_rectangle(\n",
    "            origin[0] - 15, origin[1] - 15,\n",
    "            origin[0] + 15, origin[1] + 15,\n",
    "            fill='red')\n",
    "        # return observation\n",
    "        return (np.array(self.canvas.coords(self.rect)[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT)\n",
    "\n",
    "    def step(self, action):\n",
    "        s = self.canvas.coords(self.rect)\n",
    "        base_action = np.array([0, 0])\n",
    "        if action == 0:   # up\n",
    "            if s[1] > UNIT:\n",
    "                base_action[1] -= UNIT\n",
    "        elif action == 1:   # down\n",
    "            if s[1] < (MAZE_H - 1) * UNIT:\n",
    "                base_action[1] += UNIT\n",
    "        elif action == 2:   # right\n",
    "            if s[0] < (MAZE_W - 1) * UNIT:\n",
    "                base_action[0] += UNIT\n",
    "        elif action == 3:   # left\n",
    "            if s[0] > UNIT:\n",
    "                base_action[0] -= UNIT\n",
    "\n",
    "        self.canvas.move(self.rect, base_action[0], base_action[1])  # move agent\n",
    "\n",
    "        next_coords = self.canvas.coords(self.rect)  # next state\n",
    "\n",
    "        # reward function\n",
    "        if next_coords == self.canvas.coords(self.oval):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        elif next_coords in [self.canvas.coords(self.hell1)]:\n",
    "            reward = -1\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "        s_ = (np.array(next_coords[:2]) - np.array(self.canvas.coords(self.oval)[:2]))/(MAZE_H*UNIT)\n",
    "        return s_, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        # time.sleep(0.01)\n",
    "        self.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The double DQN based on this paper: https://arxiv.org/abs/1509.06461\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "class DoubleDQN:\n",
    "    def __init__(\n",
    "            self,\n",
    "            n_actions,\n",
    "            n_features,\n",
    "            learning_rate=0.005,\n",
    "            reward_decay=0.9,\n",
    "            e_greedy=0.9,\n",
    "            replace_target_iter=200,\n",
    "            memory_size=3000,\n",
    "            batch_size=32,\n",
    "            e_greedy_increment=None,\n",
    "            output_graph=False,\n",
    "            double_q=True,\n",
    "            sess=None,\n",
    "    ):\n",
    "        self.n_actions = n_actions\n",
    "        self.n_features = n_features\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon_max = e_greedy\n",
    "        self.replace_target_iter = replace_target_iter\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon_increment = e_greedy_increment\n",
    "        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max\n",
    "\n",
    "        self.double_q = double_q    # decide to use double q or not\n",
    "\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = np.zeros((self.memory_size, n_features*2+2))\n",
    "        self._build_net()\n",
    "        t_params = tf.get_collection('target_net_params')\n",
    "        e_params = tf.get_collection('eval_net_params')\n",
    "        self.replace_target_op = [tf.assign(t, e) for t, e in zip(t_params, e_params)]\n",
    "\n",
    "        if sess is None:\n",
    "            self.sess = tf.Session()\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "        else:\n",
    "            self.sess = sess\n",
    "        if output_graph:\n",
    "            tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "        self.cost_his = []\n",
    "\n",
    "    def _build_net(self):\n",
    "        def build_layers(s, c_names, n_l1, w_initializer, b_initializer):\n",
    "            with tf.variable_scope('l1'):\n",
    "                w1 = tf.get_variable('w1', [self.n_features, n_l1], initializer=w_initializer, collections=c_names)\n",
    "                b1 = tf.get_variable('b1', [1, n_l1], initializer=b_initializer, collections=c_names)\n",
    "                l1 = tf.nn.relu(tf.matmul(s, w1) + b1)\n",
    "\n",
    "            with tf.variable_scope('l2'):\n",
    "                w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)\n",
    "                b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)\n",
    "                out = tf.matmul(l1, w2) + b2\n",
    "            return out\n",
    "        # ------------------ build evaluate_net ------------------\n",
    "        self.s = tf.placeholder(tf.float32, [None, self.n_features], name='s')  # input\n",
    "        self.q_target = tf.placeholder(tf.float32, [None, self.n_actions], name='Q_target')  # for calculating loss\n",
    "\n",
    "        with tf.variable_scope('eval_net'):\n",
    "            c_names, n_l1, w_initializer, b_initializer = \\\n",
    "                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], 20, \\\n",
    "                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers\n",
    "\n",
    "            self.q_eval = build_layers(self.s, c_names, n_l1, w_initializer, b_initializer)\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target, self.q_eval))\n",
    "        with tf.variable_scope('train'):\n",
    "            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)\n",
    "\n",
    "        # ------------------ build target_net ------------------\n",
    "        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')    # input\n",
    "        with tf.variable_scope('target_net'):\n",
    "            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]\n",
    "\n",
    "            self.q_next = build_layers(self.s_, c_names, n_l1, w_initializer, b_initializer)\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        if not hasattr(self, 'memory_counter'):\n",
    "            self.memory_counter = 0\n",
    "        transition = np.hstack((s, [a, r], s_))\n",
    "        index = self.memory_counter % self.memory_size\n",
    "        self.memory[index, :] = transition\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        observation = observation[np.newaxis, :]\n",
    "        actions_value = self.sess.run(self.q_eval, feed_dict={self.s: observation})\n",
    "        action = np.argmax(actions_value)\n",
    "\n",
    "        if not hasattr(self, 'q'):  # record action value it gets\n",
    "            self.q = []\n",
    "            self.running_q = 0\n",
    "        self.running_q = self.running_q*0.99 + 0.01 * np.max(actions_value)\n",
    "        self.q.append(self.running_q)\n",
    "\n",
    "        if np.random.uniform() > self.epsilon:  # choosing action\n",
    "            action = np.random.randint(0, self.n_actions)\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        if self.learn_step_counter % self.replace_target_iter == 0:\n",
    "            self.sess.run(self.replace_target_op)\n",
    "            print('\\ntarget_params_replaced\\n')\n",
    "\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size=self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size=self.batch_size)\n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "\n",
    "        q_next, q_eval4next = self.sess.run(\n",
    "            [self.q_next, self.q_eval],\n",
    "            feed_dict={self.s_: batch_memory[:, -self.n_features:],    # next observation\n",
    "                       self.s: batch_memory[:, -self.n_features:]})    # next observation\n",
    "        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.n_features]})\n",
    "\n",
    "        q_target = q_eval.copy()\n",
    "\n",
    "        batch_index = np.arange(self.batch_size, dtype=np.int32)\n",
    "        eval_act_index = batch_memory[:, self.n_features].astype(int)\n",
    "        reward = batch_memory[:, self.n_features + 1]\n",
    "\n",
    "        if self.double_q:\n",
    "            max_act4next = np.argmax(q_eval4next, axis=1)        # the action that brings the highest value is evaluated by q_eval\n",
    "            selected_q_next = q_next[batch_index, max_act4next]  # Double DQN, select q_next depending on above actions\n",
    "        else:\n",
    "            selected_q_next = np.max(q_next, axis=1)    # the natural DQN\n",
    "\n",
    "        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next\n",
    "\n",
    "        _, self.cost = self.sess.run([self._train_op, self.loss],\n",
    "                                     feed_dict={self.s: batch_memory[:, :self.n_features],\n",
    "                                                self.q_target: q_target})\n",
    "        self.cost_his.append(self.cost)\n",
    "\n",
    "        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max\n",
    "        self.learn_step_counter += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Double DQN & Natural DQN comparison,\n",
    "The Pendulum example.\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "Tensorflow: 1.0\n",
    "gym: 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import gym\n",
    "#from RL_brain import DoubleDQN\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "env = gym.make('Pendulum-v0')\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "MEMORY_SIZE = 3000\n",
    "ACTION_SPACE = 11\n",
    "\n",
    "sess = tf.Session()\n",
    "with tf.variable_scope('Natural_DQN'):\n",
    "    natural_DQN = DoubleDQN(\n",
    "        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,\n",
    "        e_greedy_increment=0.001, double_q=False, sess=sess\n",
    "    )\n",
    "\n",
    "with tf.variable_scope('Double_DQN'):\n",
    "    double_DQN = DoubleDQN(\n",
    "        n_actions=ACTION_SPACE, n_features=3, memory_size=MEMORY_SIZE,\n",
    "        e_greedy_increment=0.001, double_q=True, sess=sess, output_graph=True)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "def train(RL):\n",
    "    total_steps = 0\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        # if total_steps - MEMORY_SIZE > 8000: env.render()\n",
    "\n",
    "        action = RL.choose_action(observation)\n",
    "\n",
    "        f_action = (action-(ACTION_SPACE-1)/2)/((ACTION_SPACE-1)/4)   # convert to [-2 ~ 2] float actions\n",
    "        observation_, reward, done, info = env.step(np.array([f_action]))\n",
    "\n",
    "        reward /= 10     # normalize to a range of (-1, 0). r = 0 when get upright\n",
    "        # the Q target at upright state will be 0, because Q_target = r + gamma * Qmax(s', a') = 0 + gamma * 0\n",
    "        # so when Q at this state is greater than 0, the agent overestimates the Q. Please refer to the final result.\n",
    "\n",
    "        RL.store_transition(observation, action, reward, observation_)\n",
    "\n",
    "        if total_steps > MEMORY_SIZE:   # learning\n",
    "            RL.learn()\n",
    "\n",
    "        if total_steps - MEMORY_SIZE > 3:   # stop game\n",
    "            break\n",
    "\n",
    "        observation = observation_\n",
    "        total_steps += 1\n",
    "    return RL.q\n",
    "\n",
    "q_natural = train(natural_DQN)\n",
    "q_double = train(double_DQN)\n",
    "\n",
    "plt.plot(np.array(q_natural), c='r', label='natural')\n",
    "plt.plot(np.array(q_double), c='b', label='double')\n",
    "plt.legend(loc='best')\n",
    "plt.ylabel('Q eval')\n",
    "plt.xlabel('training steps')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 程序仿真部分\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "\n",
    "class Area():\n",
    "    def __init__(self, n, a_id):\n",
    "        self.a_id = a_id\n",
    "        self.normal_bike = n\n",
    "        self.broken_bike = 0\n",
    "\n",
    "    def move(self):\n",
    "        self.normal_bike -= 1\n",
    "        self.broken_bike += 1\n",
    "\n",
    "\n",
    "def binaryInsert(target, events):\n",
    "    for event in events:\n",
    "        if event >= target[-1]:\n",
    "            target.append(event)\n",
    "        else:\n",
    "            l, mid, r = 0, int(len(target) / 2), len(target) - 1\n",
    "            while 1:\n",
    "                if r - l == 1:\n",
    "                    target.insert(r, event)\n",
    "                    break\n",
    "                else:\n",
    "                    if event > target[mid]:\n",
    "                        l = mid\n",
    "                        mid = int((r + l) / 2)\n",
    "                    else:\n",
    "                        r = mid\n",
    "                        mid = int((r + l) / 2)\n",
    "\n",
    "\n",
    "def simulate(N, R, A, Q, P, t_limit):\n",
    "    '''\n",
    "    N: initial number of bikes in the network\n",
    "    R: rates of arriving and departing for each node\n",
    "    A: number of Areas\n",
    "    Q: matrix of tansitition probability\n",
    "    P: policy of dealing with broken bikes\n",
    "\n",
    "    T: system clock\n",
    "    scheduler: stack of upcoming events,[t, event_type, area], 1 for customer arrival, 2 for bike arrival\n",
    "    \n",
    "    stat: record of system state parameters\n",
    "    loss: record of customer loss\n",
    "    '''\n",
    "\n",
    "    # initiate\n",
    "    T = 0\n",
    "    #stat, S = pd.DataFrame(columns=['type', 'place', 't']), 0\n",
    "    loss, L = pd.DataFrame(columns=['place', 't']), 0\n",
    "    broken, B = pd.DataFrame(columns=['place', 'ng', 'nb', 't']), 0\n",
    "\n",
    "    # initiation of instances of Area and scheduler\n",
    "    scheduler = []\n",
    "    a = []\n",
    "    for i in range(A):\n",
    "        a.append(Area(N / A, i))\n",
    "        scheduler.append([np.random.exponential(1/R.loc[i].cus_arr), 1, a[i]])\n",
    "    scheduler.sort()\n",
    "\n",
    "    # system running\n",
    "    while T < t_limit:\n",
    "        \n",
    "        T = scheduler[0][0]\n",
    "        if scheduler[0][1] == 1:\n",
    "            #stat.loc[S], S = [scheduler[0][1], scheduler[0][2].a_id, T], S+1\n",
    "            if scheduler[0][2].normal_bike == 0:\n",
    "                # this is a loss\n",
    "                loss.loc[L], L = [scheduler[0][2].a_id, T], L+1\n",
    "                event = [T + np.random.exponential(1/R.loc[scheduler[0][2].a_id].cus_arr), 1, scheduler[0][2]]\n",
    "                binaryInsert(scheduler, [event])\n",
    "            else:\n",
    "                target = np.random.choice(np.arange(A+1), 1, p=Q[scheduler[0][2].a_id])\n",
    "                if target == A:\n",
    "                    broken.loc[B], B = [scheduler[0][2].a_id, scheduler[0][2].normal_bike, scheduler[0][2].broken_bike, T], B+1\n",
    "                    scheduler[0][2].move()\n",
    "                    continue\n",
    "                else:\n",
    "                    scheduler[0][2].normal_bike -= 1\n",
    "                    event1 = [T + np.random.exponential(1/R.loc[scheduler[0][2].a_id].ride), 2, a[target[0]]]\n",
    "                    event2 = [T + np.random.exponential(1/R.loc[scheduler[0][2].a_id].cus_arr), 1, scheduler[0][2]]\n",
    "                    binaryInsert(scheduler, [event1, event2])\n",
    "        else:\n",
    "            #stat.loc[S], S = [scheduler[0][1], scheduler[0][2].a_id, T], S+1\n",
    "            scheduler[0][2].normal_bike += 1\n",
    "        scheduler.pop(0)\n",
    "        \n",
    "    #return stat\n",
    "    return loss, broken\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1)\n",
    "    N = 360\n",
    "    A = 9\n",
    "    R = pd.DataFrame({'cus_arr': [5] * A, 'ride': [10] * A}, index=range(A))\n",
    "    Q = [np.random.rand(A+1) for i in range(A)]\n",
    "    Q = [q / sum(q) for q in Q]\n",
    "    #Q = [[0,0.9,0.1], [0.9,0,0.1]]\n",
    "    P = 0\n",
    "    time_limit = 6000\n",
    "\n",
    "    result = simulate(N, R, A, Q, P, time_limit)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
